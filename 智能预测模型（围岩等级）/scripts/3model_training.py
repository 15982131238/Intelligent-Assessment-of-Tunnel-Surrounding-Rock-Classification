# -*- coding: utf-8 -*-
"""
éš§é“æŒå­é¢å›´å²©ç­‰çº§æ™ºèƒ½è¯„ä¼°å®Œæ•´è®­ç»ƒç³»ç»Ÿï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰
åŠŸèƒ½ï¼šèŠ‚ç†åˆ†å‰² + è£‚éš™åˆ†å‰² + å›´å²©ç­‰çº§æ™ºèƒ½é¢„æµ‹ï¼ˆ15çº§è¯¦ç»†åˆ†ç±»ï¼‰
åŸºäºCOCOæ ¼å¼æ ‡æ³¨æ•°æ®ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å›´å²©ç­‰çº§è¯„ä¼°
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2024
"""

import os
import json
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights  # ä½¿ç”¨æ–°çš„æƒé‡API
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import logging
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RockQualityConfig:
    """å›´å²©ç­‰çº§è¯„ä¼°é…ç½®ç±»ï¼ˆ15çº§è¯¦ç»†åˆ†ç±»ï¼‰"""
    def __init__(self):
        # åŸºç¡€è·¯å¾„é…ç½®
        self.data_root = r"c:\Users\ASUS\Desktop\AI_Recognition"
        self.coco_file = os.path.join(self.data_root, "annotations", "instances_default.json")
        self.images_dir = os.path.join(self.data_root, "tunnel_face_images")
        self.output_dir = os.path.join(self.data_root, "processed_tunnel_face_photos", "3")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "checkpoints"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "visualizations"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "reports"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "predictions"), exist_ok=True)
        
        # å›´å²©ç­‰çº§å®šä¹‰ï¼ˆ15çº§è¯¦ç»†åˆ†ç±»ï¼‰
        self.rock_quality_levels = {
            0: 'â… çº§å¼º', 1: 'â… çº§ä¸­', 2: 'â… çº§å¼±',
            3: 'â…¡çº§å¼º', 4: 'â…¡çº§ä¸­', 5: 'â…¡çº§å¼±',
            6: 'â…¢çº§å¼º', 7: 'â…¢çº§ä¸­', 8: 'â…¢çº§å¼±',
            9: 'â…£çº§å¼º', 10: 'â…£çº§ä¸­', 11: 'â…£çº§å¼±',
            12: 'â…¤çº§å¼º', 13: 'â…¤çº§ä¸­', 14: 'â…¤çº§å¼±'
        }
        
        # å›´å²©ç­‰çº§è¯„ä¼°æ ‡å‡†ï¼ˆåŸºäºç”¨æˆ·æä¾›çš„è¯¦ç»†æ ‡å‡†ï¼‰
        self.evaluation_criteria = {
            # â… çº§å¼º
            0: {
                'joint_line_length': (0, 10),      # é•¿åº¦ï¼œ10cm
                'joint_spacing': (50, float('inf')), # ï¼50cm
                'joint_thickness': (0, 10),         # æè–„å±‚ï¼ˆï¼œ10cmï¼‰
                'crack_length': (0, 5),             # é•¿åº¦ï¼œ5cm
                'crack_distance': (30, float('inf')), # ï¼30cm
                'weathering_level': 'full',          # å…¨é£åŒ–
                'moisture_level': 'dry'              # å¹²ç‡¥
            },
            # â… çº§ä¸­
            1: {
                'joint_line_length': (10, 30),      # é•¿åº¦10-30cm
                'joint_spacing': (30, 50),          # 30-50cm
                'joint_thickness': (10, 30),        # è–„å±‚ï¼ˆ10-30cmï¼‰
                'crack_length': (5, 15),            # é•¿åº¦5-15cm
                'crack_distance': (20, 30),         # 20-30cm
                'weathering_level': 'strong',        # å¼ºé£åŒ–
                'moisture_level': 'slightly_wet'     # å¾®æ½®
            },
            # â… çº§å¼±
            2: {
                'joint_line_length': (30, float('inf')), # é•¿åº¦ï¼30cm
                'joint_spacing': (10, 30),          # 10-30cm
                'joint_thickness': (30, 100),       # ä¸­åšå±‚ï¼ˆ30-100cmï¼‰
                'crack_length': (15, float('inf')), # é•¿åº¦ï¼15cm
                'crack_distance': (10, 20),         # 10-20cm
                'weathering_level': 'moderate',      # ä¸­ç­‰é£åŒ–
                'moisture_level': 'slightly_wet'     # å¾®æ½®
            },
            # â…¡çº§å¼º
            3: {
                'joint_line_length': (0, 15),       # é•¿åº¦ï¼œ15cm
                'joint_spacing': (40, float('inf')), # ï¼40cm
                'joint_thickness': (0, 10),         # æè–„å±‚ï¼ˆï¼œ10cmï¼‰
                'crack_length': (0, 8),             # é•¿åº¦ï¼œ8cm
                'crack_distance': (25, float('inf')), # ï¼25cm
                'weathering_level': 'moderate',      # ä¸­ç­‰é£åŒ–
                'moisture_level': 'slightly_wet'     # å¾®æ½®
            },
            # â…¡çº§ä¸­
            4: {
                'joint_line_length': (15, 35),      # é•¿åº¦15-35cm
                'joint_spacing': (20, 40),          # 20-40cm
                'joint_thickness': (10, 30),        # è–„å±‚ï¼ˆ10-30cmï¼‰
                'crack_length': (8, 20),            # é•¿åº¦8-20cm
                'crack_distance': (15, 25),         # 15-25cm
                'weathering_level': 'slight_to_moderate', # è½»å¾®è‡³ä¸­ç­‰é£åŒ–
                'moisture_level': 'slightly_wet'     # å¾®æ½®
            },
            # â…¡çº§å¼±
            5: {
                'joint_line_length': (35, float('inf')), # é•¿åº¦ï¼35cm
                'joint_spacing': (10, 20),          # 10-20cm
                'joint_thickness': (30, 100),       # ä¸­åšå±‚ï¼ˆ30-100cmï¼‰
                'crack_length': (20, float('inf')), # é•¿åº¦ï¼20cm
                'crack_distance': (10, 15),         # 10-15cm
                'weathering_level': 'slight',        # è½»å¾®é£åŒ–
                'moisture_level': 'slightly_wet_local' # å¾®æ½®æˆ–å±€éƒ¨æ¹¿æ¶¦
            },
            # â…¢çº§å¼º
            6: {
                'joint_line_length': (0, 20),       # é•¿åº¦ï¼œ20cm
                'joint_spacing': (30, float('inf')), # ï¼30cm
                'joint_thickness': (0, 10),         # æè–„å±‚ï¼ˆï¼œ10cmï¼‰
                'crack_length': (0, 10),            # é•¿åº¦ï¼œ10cm
                'crack_distance': (20, float('inf')), # ï¼20cm
                'weathering_level': 'moderate',      # ä¸­ç­‰é£åŒ–
                'moisture_level': 'slightly_wet_local' # å¾®æ½®æˆ–å±€éƒ¨æ¹¿æ¶¦
            },
            # â…¢çº§ä¸­
            7: {
                'joint_line_length': (20, 40),      # é•¿åº¦20-40cm
                'joint_spacing': (15, 30),          # 15-30cm
                'joint_thickness': (10, 30),        # è–„å±‚ï¼ˆ10-30cmï¼‰
                'crack_length': (10, 30),           # é•¿åº¦10-30cm
                'crack_distance': (10, 20),         # 10-20cm
                'weathering_level': 'moderate',      # ä¸­ç­‰é£åŒ–
                'moisture_level': 'local_wet'        # å±€éƒ¨æ¹¿æ¶¦
            },
            # â…¢çº§å¼±
            8: {
                'joint_line_length': (40, float('inf')), # é•¿åº¦ï¼40cm
                'joint_spacing': (0, 15),           # ï¼œ15cm
                'joint_thickness': (30, 100),       # ä¸­åšå±‚ï¼ˆ30-100cmï¼‰
                'crack_length': (30, float('inf')), # é•¿åº¦ï¼30cm
                'crack_distance': (0, 10),          # ï¼œ10cm
                'weathering_level': 'moderate_to_strong', # ä¸­ç­‰è‡³å¼ºé£åŒ–
                'moisture_level': 'local_wet'        # å±€éƒ¨æ¹¿æ¶¦
            },
            # â…£çº§å¼º
            9: {
                'joint_line_length': (0, 25),       # é•¿åº¦ï¼œ25cm
                'joint_spacing': (25, float('inf')), # ï¼25cm
                'joint_thickness': (0, 10),         # æè–„å±‚ï¼ˆï¼œ10cmï¼‰
                'crack_length': (0, 15),            # é•¿åº¦ï¼œ15cm
                'crack_distance': (15, float('inf')), # ï¼15cm
                'weathering_level': 'strong',        # å¼ºé£åŒ–
                'moisture_level': 'local_wet_drip'   # å±€éƒ¨æ¹¿æ¶¦æˆ–æ»´æ°´
            },
            # â…£çº§ä¸­
            10: {
                'joint_line_length': (25, 50),      # é•¿åº¦25-50cm
                'joint_spacing': (10, 25),          # 10-25cm
                'joint_thickness': (10, 30),        # è–„å±‚ï¼ˆ10-30cmï¼‰
                'crack_length': (15, 40),           # é•¿åº¦15-40cm
                'crack_distance': (5, 15),          # 5-15cm
                'weathering_level': 'strong',        # å¼ºé£åŒ–
                'moisture_level': 'local_wet_drip'   # å±€éƒ¨æ¹¿æ¶¦æˆ–æ»´æ°´
            },
            # â…£çº§å¼±
            11: {
                'joint_line_length': (50, float('inf')), # é•¿åº¦ï¼50cm
                'joint_spacing': (0, 10),           # ï¼œ10cm
                'joint_thickness': (30, 100),       # ä¸­åšå±‚ï¼ˆ30-100cmï¼‰
                'crack_length': (40, float('inf')), # é•¿åº¦ï¼40cm
                'crack_distance': (0, 5),           # ï¼œ5cm
                'weathering_level': 'strong_to_full', # å¼ºé£åŒ–è‡³å…¨é£åŒ–
                'moisture_level': 'wet_drip'         # æ¹¿æ¶¦æˆ–æ»´æ°´
            },
            # â…¤çº§å¼º
            12: {
                'joint_line_length': (0, 30),       # é•¿åº¦ï¼œ30cm
                'joint_spacing': (20, float('inf')), # ï¼20cm
                'joint_thickness': (0, 10),         # æè–„å±‚ï¼ˆï¼œ10cmï¼‰
                'crack_length': (0, 20),            # é•¿åº¦ï¼œ20cm
                'crack_distance': (15, float('inf')), # ï¼15cm
                'weathering_level': 'full',          # å…¨é£åŒ–
                'moisture_level': 'wet_drip'         # æ¹¿æ¶¦æˆ–æ»´æ°´
            },
            # â…¤çº§ä¸­
            13: {
                'joint_line_length': (30, 60),      # é•¿åº¦30-60cm
                'joint_spacing': (10, 20),          # 10-20cm
                'joint_thickness': (10, 30),        # è–„å±‚ï¼ˆ10-30cmï¼‰
                'crack_length': (20, 50),           # é•¿åº¦20-50cm
                'crack_distance': (5, 15),          # 5-15cm
                'weathering_level': 'full',          # å…¨é£åŒ–
                'moisture_level': 'wet_drip'         # æ¹¿æ¶¦æˆ–æ»´æ°´
            },
            # â…¤çº§å¼±
            14: {
                'joint_line_length': (60, float('inf')), # é•¿åº¦ï¼60cm
                'joint_spacing': (0, 10),           # ï¼œ10cm
                'joint_thickness': (30, 100),       # ä¸­åšå±‚ï¼ˆ30-100cmï¼‰
                'crack_length': (50, float('inf')), # é•¿åº¦ï¼50cm
                'crack_distance': (0, 5),           # ï¼œ5cm
                'weathering_level': 'full',          # å…¨é£åŒ–
                'moisture_level': 'drip_flow'        # æ»´æ°´æˆ–æµæ°´
            }
        }
        
        # æ¨¡å‹é…ç½®
        self.num_seg_classes = 7  # èƒŒæ™¯+6ä¸ªåœ°è´¨ç‰¹å¾ç±»åˆ«
        self.num_quality_classes = 15  # 15çº§è¯¦ç»†åˆ†ç±»
        self.input_size = (512, 512)
        
        # è®­ç»ƒé…ç½®
        self.batch_size = 2  # è¿›ä¸€æ­¥å‡å°batch size
        self.num_epochs = 50
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.patience = 10
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_workers = 0  # è®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        logger.info(f"å›´å²©ç­‰çº§ç±»åˆ«ï¼ˆ15çº§ï¼‰: {self.rock_quality_levels}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")

class AdvancedRockQualityEvaluator:
    """é«˜çº§å›´å²©ç­‰çº§æ™ºèƒ½è¯„ä¼°å™¨ï¼ˆ15çº§è¯¦ç»†åˆ†ç±»ï¼‰"""
    def __init__(self, config):
        self.config = config
        self.criteria = config.evaluation_criteria
        
    def extract_joint_features(self, joint_mask):
        """æå–èŠ‚ç†ç‰¹å¾"""
        if isinstance(joint_mask, torch.Tensor):
            joint_mask = joint_mask.cpu().numpy()
        
        features = {
            'line_length': 0,
            'spacing': 0,
            'thickness': 0,
            'density': 0
        }
        
        if np.sum(joint_mask) > 0:
            # è®¡ç®—èŠ‚ç†çº¿é•¿åº¦
            contours, _ = cv2.findContours(joint_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                total_length = sum([cv2.arcLength(contour, False) for contour in contours])
                features['line_length'] = total_length * 0.1  # åƒç´ è½¬cmï¼ˆå‡è®¾æ¯”ä¾‹ï¼‰
                
                # è®¡ç®—èŠ‚ç†é—´è·ï¼ˆåŸºäºè½®å»“é—´è·ç¦»ï¼‰
                if len(contours) > 1:
                    distances = []
                    for i in range(len(contours)-1):
                        for j in range(i+1, len(contours)):
                            dist = cv2.pointPolygonTest(contours[i], tuple(contours[j][0][0]), True)
                            distances.append(abs(dist))
                    features['spacing'] = np.mean(distances) * 0.1 if distances else 100
                else:
                    features['spacing'] = 100  # å•ä¸ªèŠ‚ç†ï¼Œé—´è·å¤§
                
                # è®¡ç®—èŠ‚ç†åšåº¦ï¼ˆåŸºäºæ©ç å®½åº¦ï¼‰
                kernel = np.ones((3,3), np.uint8)
                dilated = cv2.dilate(joint_mask.astype(np.uint8), kernel, iterations=1)
                thickness_map = cv2.distanceTransform(dilated, cv2.DIST_L2, 5)
                features['thickness'] = np.max(thickness_map) * 0.2  # åƒç´ è½¬cm
        
        return features
    
    def extract_crack_features(self, crack_mask):
        """æå–è£‚éš™ç‰¹å¾"""
        if isinstance(crack_mask, torch.Tensor):
            crack_mask = crack_mask.cpu().numpy()
        
        features = {
            'length': 0,
            'distance': 0,
            'depth': 'shallow',
            'opening': 'closed'
        }
        
        if np.sum(crack_mask) > 0:
            contours, _ = cv2.findContours(crack_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                # è®¡ç®—è£‚éš™é•¿åº¦
                total_length = sum([cv2.arcLength(contour, False) for contour in contours])
                features['length'] = total_length * 0.1
                
                # è®¡ç®—è£‚éš™é—´è·
                if len(contours) > 1:
                    distances = []
                    for i in range(len(contours)-1):
                        for j in range(i+1, len(contours)):
                            dist = cv2.pointPolygonTest(contours[i], tuple(contours[j][0][0]), True)
                            distances.append(abs(dist))
                    features['distance'] = np.mean(distances) * 0.1 if distances else 50
                else:
                    features['distance'] = 50
                
                # åŸºäºé¢ç§¯ä¼°ç®—æ·±åº¦å’Œå¼ å¼€åº¦
                total_area = np.sum(crack_mask)
                if total_area > 1000:
                    features['depth'] = 'deep'
                    features['opening'] = 'wide'
                elif total_area > 500:
                    features['depth'] = 'medium'
                    features['opening'] = 'moderate'
        
        return features
    
    def evaluate_weathering(self, weathering_mask):
        """è¯„ä¼°é£åŒ–ç¨‹åº¦"""
        if isinstance(weathering_mask, torch.Tensor):
            weathering_mask = weathering_mask.cpu().numpy()
        
        weathering_ratio = np.sum(weathering_mask) / (weathering_mask.shape[0] * weathering_mask.shape[1])
        
        if weathering_ratio > 0.7:
            return 'full'
        elif weathering_ratio > 0.5:
            return 'strong_to_full'
        elif weathering_ratio > 0.3:
            return 'strong'
        elif weathering_ratio > 0.15:
            return 'moderate_to_strong'
        elif weathering_ratio > 0.05:
            return 'moderate'
        elif weathering_ratio > 0.01:
            return 'slight_to_moderate'
        else:
            return 'slight'
    
    def evaluate_moisture(self, moisture_mask):
        """è¯„ä¼°æ¹¿æ¶¦ç¨‹åº¦"""
        if isinstance(moisture_mask, torch.Tensor):
            moisture_mask = moisture_mask.cpu().numpy()
        
        moisture_ratio = np.sum(moisture_mask) / (moisture_mask.shape[0] * moisture_mask.shape[1])
        
        if moisture_ratio > 0.3:
            return 'drip_flow'
        elif moisture_ratio > 0.2:
            return 'wet_drip'
        elif moisture_ratio > 0.1:
            return 'local_wet_drip'
        elif moisture_ratio > 0.05:
            return 'local_wet'
        elif moisture_ratio > 0.01:
            return 'slightly_wet_local'
        elif moisture_ratio > 0.005:
            return 'slightly_wet'
        else:
            return 'dry'
    
    def comprehensive_evaluation(self, feature_masks):
        """ç»¼åˆè¯„ä¼°å›´å²©ç­‰çº§ï¼ˆ15çº§è¯¦ç»†åˆ†ç±»ï¼‰"""
        # æå–å„ç±»ç‰¹å¾
        joint_features = self.extract_joint_features(feature_masks.get('joint', np.zeros((512, 512))))
        crack_features = self.extract_crack_features(feature_masks.get('crack', np.zeros((512, 512))))
        weathering_level = self.evaluate_weathering(feature_masks.get('weathering', np.zeros((512, 512))))
        moisture_level = self.evaluate_moisture(feature_masks.get('moisture', np.zeros((512, 512))))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æº¶æ´ï¼ˆç›´æ¥åˆ¤å®šä¸ºæœ€å·®ç­‰çº§ï¼‰
        karst_area = np.sum(feature_masks.get('karst', np.zeros((512, 512))) > 0)
        if karst_area > 100:  # æœ‰æ˜æ˜¾æº¶æ´
            return {
                'rock_quality': 14,  # â…¤çº§å¼±
                'confidence': 0.95,
                'reason': 'æ£€æµ‹åˆ°æº¶æ´ï¼Œç›´æ¥åˆ¤å®šä¸ºâ…¤çº§å¼±',
                'features': {
                    'joint': joint_features,
                    'crack': crack_features,
                    'weathering': weathering_level,
                    'moisture': moisture_level,
                    'karst': True
                }
            }
        
        # åŸºäºç‰¹å¾åŒ¹é…è¯„ä¼°ç­‰çº§
        best_match = 0
        best_score = 0
        
        for level, criteria in self.criteria.items():
            score = 0
            total_criteria = 0
            
            # èŠ‚ç†çº¿é•¿åº¦åŒ¹é…
            if criteria['joint_line_length'][0] <= joint_features['line_length'] <= criteria['joint_line_length'][1]:
                score += 1
            total_criteria += 1
            
            # èŠ‚ç†é—´è·åŒ¹é…
            if criteria['joint_spacing'][0] <= joint_features['spacing'] <= criteria['joint_spacing'][1]:
                score += 1
            total_criteria += 1
            
            # èŠ‚ç†åšåº¦åŒ¹é…
            if criteria['joint_thickness'][0] <= joint_features['thickness'] <= criteria['joint_thickness'][1]:
                score += 1
            total_criteria += 1
            
            # è£‚éš™é•¿åº¦åŒ¹é…
            if criteria['crack_length'][0] <= crack_features['length'] <= criteria['crack_length'][1]:
                score += 1
            total_criteria += 1
            
            # è£‚éš™è·ç¦»åŒ¹é…
            if criteria['crack_distance'][0] <= crack_features['distance'] <= criteria['crack_distance'][1]:
                score += 1
            total_criteria += 1
            
            # é£åŒ–ç¨‹åº¦åŒ¹é…
            if criteria['weathering_level'] == weathering_level:
                score += 2  # é£åŒ–ç¨‹åº¦æƒé‡æ›´é«˜
            total_criteria += 2
            
            # æ¹¿æ¶¦ç¨‹åº¦åŒ¹é…
            if criteria['moisture_level'] == moisture_level:
                score += 2  # æ¹¿æ¶¦ç¨‹åº¦æƒé‡æ›´é«˜
            total_criteria += 2
            
            # è®¡ç®—åŒ¹é…åº¦
            match_score = score / total_criteria if total_criteria > 0 else 0
            
            if match_score > best_score:
                best_score = match_score
                best_match = level
        
        return {
            'rock_quality': best_match,
            'confidence': best_score,
            'level_name': self.config.rock_quality_levels[best_match],
            'features': {
                'joint': joint_features,
                'crack': crack_features,
                'weathering': weathering_level,
                'moisture': moisture_level,
                'karst': False
            }
        }

class TunnelDataset(Dataset):
    """éš§é“æŒå­é¢æ•°æ®é›†"""
    def __init__(self, coco_file, images_dir, target_size=(512, 512), transform=None):
        self.images_dir = images_dir
        self.target_size = target_size
        self.transform = transform
        self.evaluator = AdvancedRockQualityEvaluator(RockQualityConfig())
        
        # åŠ è½½COCOæ ‡æ³¨
        try:
            with open(coco_file, 'r', encoding='utf-8') as f:
                self.coco_data = json.load(f)
        except Exception as e:
            logger.error(f"æ— æ³•åŠ è½½COCOæ–‡ä»¶: {e}")
            raise
        
        # è§£ææ•°æ®
        self.images = {img['id']: img for img in self.coco_data['images']}
        self.annotations = self.coco_data['annotations']
        self.categories = {cat['id']: cat['name'] for cat in self.coco_data['categories']}
        
        # æŒ‰å›¾åƒIDåˆ†ç»„æ ‡æ³¨
        self.image_annotations = {}
        for ann in self.annotations:
            img_id = ann['image_id']
            if img_id not in self.image_annotations:
                self.image_annotations[img_id] = []
            self.image_annotations[img_id].append(ann)
        
        self.image_ids = list(self.images.keys())
        
        logger.info(f"åŠ è½½æ•°æ®é›†: {len(self.image_ids)} å¼ å›¾åƒ")
        logger.info(f"ç±»åˆ«: {self.categories}")
    
    def __len__(self):
        return len(self.image_ids)
    
    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.images[img_id]
        
        # åŠ è½½å›¾åƒ
        img_path = os.path.join(self.images_dir, img_info['file_name'])
        try:
            image = cv2.imread(img_path)
            if image is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {img_path}")
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        except Exception as e:
            logger.warning(f"å›¾åƒåŠ è½½å¤±è´¥ {img_path}: {e}ï¼Œä½¿ç”¨é»˜è®¤å›¾åƒ")
            image = np.zeros((512, 512, 3), dtype=np.uint8)
        
        original_size = image.shape[:2]
        image = cv2.resize(image, self.target_size)
        
        # åˆ›å»ºå„ç±»ç‰¹å¾æ©ç 
        feature_masks = {
            'joint': np.zeros(self.target_size, dtype=np.uint8),
            'crack': np.zeros(self.target_size, dtype=np.uint8),
            'weathering': np.zeros(self.target_size, dtype=np.uint8),
            'moisture': np.zeros(self.target_size, dtype=np.uint8),
            'karst': np.zeros(self.target_size, dtype=np.uint8)
        }
        
        # å¤„ç†æ ‡æ³¨
        if img_id in self.image_annotations:
            for ann in self.image_annotations[img_id]:
                category_name = self.categories[ann['category_id']]
                mask = self._create_mask_from_annotation(ann, original_size, self.target_size)
                
                # æ ¹æ®ç±»åˆ«åç§°åˆ†é…åˆ°å¯¹åº”çš„ç‰¹å¾æ©ç 
                if 'èŠ‚ç†' in category_name:
                    feature_masks['joint'] = np.maximum(feature_masks['joint'], mask)
                elif 'è£‚ç¼' in category_name or 'è£‚éš™' in category_name:
                    feature_masks['crack'] = np.maximum(feature_masks['crack'], mask)
                elif 'é£åŒ–' in category_name:
                    feature_masks['weathering'] = np.maximum(feature_masks['weathering'], mask)
                elif 'æ¹¿æ¶¦' in category_name:
                    feature_masks['moisture'] = np.maximum(feature_masks['moisture'], mask)
                elif 'æº¶æ´' in category_name:
                    feature_masks['karst'] = np.maximum(feature_masks['karst'], mask)
        
        # æ™ºèƒ½è¯„ä¼°å›´å²©ç­‰çº§
        evaluation_result = self.evaluator.comprehensive_evaluation(feature_masks)
        rock_quality = evaluation_result['rock_quality']
        
        # åˆ›å»ºç»¼åˆåˆ†å‰²æ©ç 
        seg_mask = np.zeros(self.target_size, dtype=np.uint8)
        for i, (feature_type, mask) in enumerate(feature_masks.items(), 1):
            seg_mask[mask > 0] = i
        
        # è½¬æ¢ä¸ºå¼ é‡
        if self.transform:
            image = self.transform(image)
        else:
            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
        
        seg_mask = torch.from_numpy(seg_mask).long()
        rock_quality = torch.tensor(rock_quality, dtype=torch.long)
        
        return {
            'image': image,
            'seg_mask': seg_mask,
            'rock_quality': rock_quality,
            'image_id': img_id,
            'evaluation_details': evaluation_result
        }
    
    def _create_mask_from_annotation(self, ann, original_size, target_size):
        """ä»COCOæ ‡æ³¨åˆ›å»ºæ©ç """
        mask = np.zeros(original_size, dtype=np.uint8)
        
        try:
            if 'segmentation' in ann and ann['segmentation']:
                for seg in ann['segmentation']:
                    if isinstance(seg, list) and len(seg) >= 6:
                        poly = np.array(seg).reshape(-1, 2).astype(np.int32)
                        cv2.fillPoly(mask, [poly], 1)
            elif 'bbox' in ann:
                x, y, w, h = ann['bbox']
                mask[int(y):int(y+h), int(x):int(x+w)] = 1
        except Exception as e:
            logger.warning(f"åˆ›å»ºæ©ç å¤±è´¥: {e}")
        
        if original_size != target_size:
            mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)
        
        return mask

class AttentionModule(nn.Module):
    """æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, channels):
        super(AttentionModule, self).__init__()
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // 8, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 8, channels, 1),
            nn.Sigmoid()
        )
        
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, 7, padding=3),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # é€šé“æ³¨æ„åŠ›
        ca = self.channel_attention(x)
        x = x * ca
        
        # ç©ºé—´æ³¨æ„åŠ›
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        spatial_input = torch.cat([avg_pool, max_pool], dim=1)
        sa = self.spatial_attention(spatial_input)
        x = x * sa
        
        return x

class RockQualityNet(nn.Module):
    """å›´å²©ç­‰çº§è¯„ä¼°ç½‘ç»œï¼ˆä½¿ç”¨ResNet50éª¨å¹²ç½‘ç»œï¼Œä¿®å¤ç‰ˆï¼‰"""
    def __init__(self, num_seg_classes=7, num_quality_classes=15):
        super(RockQualityNet, self).__init__()
        
        # ä½¿ç”¨æ–°çš„æƒé‡APIé¿å…å“ˆå¸Œé—®é¢˜
        try:
            # å°è¯•ä½¿ç”¨æ–°çš„æƒé‡API
            backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
        except:
            # å¦‚æœæ–°APIä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§APIä½†è·³è¿‡å“ˆå¸Œæ£€æŸ¥
            try:
                import torch.hub
                # ä¸´æ—¶ç¦ç”¨å“ˆå¸Œæ£€æŸ¥
                original_check_hash = torch.hub.download_url_to_file
                def patched_download(url, dst, hash_prefix=None, progress=True):
                    return original_check_hash(url, dst, hash_prefix=None, progress=progress)
                torch.hub.download_url_to_file = patched_download
                
                backbone = resnet50(pretrained=True)
                
                # æ¢å¤åŸå§‹å‡½æ•°
                torch.hub.download_url_to_file = original_check_hash
            except:
                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
                logger.warning("æ— æ³•åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                backbone = resnet50(pretrained=False)
        
        self.backbone_features = nn.Sequential(
            backbone.conv1,
            backbone.bn1,
            backbone.relu,
            backbone.maxpool,
            backbone.layer1,
            backbone.layer2,
            backbone.layer3,
            backbone.layer4
        )
        
        backbone_channels = 2048
        
        # æ³¨æ„åŠ›æ¨¡å—
        self.attention = AttentionModule(backbone_channels)
        
        # åˆ†å‰²å¤´
        self.seg_head = nn.Sequential(
            nn.Conv2d(backbone_channels, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_seg_classes, 1)
        )
        
        # å›´å²©ç­‰çº§åˆ†ç±»å¤´ï¼ˆ15çº§åˆ†ç±»ï¼‰
        self.quality_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(backbone_channels, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, num_quality_classes)
        )
        
        # è¾¹ç•Œæ£€æµ‹å¤´
        self.boundary_head = nn.Sequential(
            nn.Conv2d(backbone_channels, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # æå–ç‰¹å¾
        features = self.backbone_features(x)
        
        # åº”ç”¨æ³¨æ„åŠ›
        features = self.attention(features)
        
        # åˆ†å‰²é¢„æµ‹
        seg_output = self.seg_head(features)
        seg_output = F.interpolate(seg_output, size=(512, 512), mode='bilinear', align_corners=False)
        
        # è¾¹ç•Œé¢„æµ‹
        boundary_output = self.boundary_head(features)
        boundary_output = F.interpolate(boundary_output, size=(512, 512), mode='bilinear', align_corners=False)
        
        # å›´å²©ç­‰çº§é¢„æµ‹
        quality_output = self.quality_head(features)
        
        return {
            'segmentation': seg_output,
            'boundary': boundary_output,
            'rock_quality': quality_output
        }

class CombinedLoss(nn.Module):
    """ç»„åˆæŸå¤±å‡½æ•°"""
    def __init__(self, seg_weight=1.0, boundary_weight=0.5, quality_weight=2.0):
        super(CombinedLoss, self).__init__()
        self.seg_weight = seg_weight
        self.boundary_weight = boundary_weight
        self.quality_weight = quality_weight
        
        self.seg_loss = nn.CrossEntropyLoss()
        self.boundary_loss = nn.BCELoss()
        self.quality_loss = nn.CrossEntropyLoss()
    
    def forward(self, outputs, targets):
        # åˆ†å‰²æŸå¤±
        seg_loss = self.seg_loss(outputs['segmentation'], targets['seg_mask'])
        
        # è¾¹ç•ŒæŸå¤±
        boundary_target = self._extract_boundaries(targets['seg_mask'])
        boundary_loss = self.boundary_loss(outputs['boundary'].squeeze(1), boundary_target.float())
        
        # å›´å²©ç­‰çº§æŸå¤±
        quality_loss = self.quality_loss(outputs['rock_quality'], targets['rock_quality'])
        
        total_loss = (
            self.seg_weight * seg_loss +
            self.boundary_weight * boundary_loss +
            self.quality_weight * quality_loss
        )
        
        return {
            'total_loss': total_loss,
            'seg_loss': seg_loss,
            'boundary_loss': boundary_loss,
            'quality_loss': quality_loss
        }
    
    def _extract_boundaries(self, seg_mask):
        """ä»åˆ†å‰²æ©ç æå–è¾¹ç•Œ"""
        boundaries = torch.zeros_like(seg_mask, dtype=torch.float)
        
        for i in range(seg_mask.shape[0]):
            mask = seg_mask[i].cpu().numpy().astype(np.uint8)
            boundary = cv2.Canny(mask * 50, 50, 150) > 0
            boundaries[i] = torch.from_numpy(boundary.astype(np.float32))
        
        return boundaries

class RockQualityTrainer:
    """å›´å²©ç­‰çº§è®­ç»ƒå™¨"""
    def __init__(self, config):
        self.config = config
        self.device = config.device
        
        # åˆ›å»ºæ¨¡å‹
        self.model = RockQualityNet(
            num_seg_classes=config.num_seg_classes,
            num_quality_classes=config.num_quality_classes
        ).to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.criterion = CombinedLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [], 'seg_loss': [], 'boundary_loss': [], 'quality_loss': [],
            'val_loss': [], 'val_accuracy': []
        }
        
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_seg_loss = 0
        total_boundary_loss = 0
        total_quality_loss = 0
        
        pbar = tqdm(train_loader, desc='Training')
        for batch_idx, batch in enumerate(pbar):
            try:
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                images = batch['image'].to(self.device)
                seg_masks = batch['seg_mask'].to(self.device)
                rock_qualities = batch['rock_quality'].to(self.device)
                
                targets = {
                    'seg_mask': seg_masks,
                    'rock_quality': rock_qualities
                }
                
                # å‰å‘ä¼ æ’­
                with autocast():
                    outputs = self.model(images)
                    loss_dict = self.criterion(outputs, targets)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                self.scaler.scale(loss_dict['total_loss']).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
                
                # è®°å½•æŸå¤±
                total_loss += loss_dict['total_loss'].item()
                total_seg_loss += loss_dict['seg_loss'].item()
                total_boundary_loss += loss_dict['boundary_loss'].item()
                total_quality_loss += loss_dict['quality_loss'].item()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f"{loss_dict['total_loss'].item():.4f}",
                    'Seg': f"{loss_dict['seg_loss'].item():.4f}",
                    'Quality': f"{loss_dict['quality_loss'].item():.4f}"
                })
                
            except Exception as e:
                logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
        
        avg_loss = total_loss / len(train_loader)
        avg_seg_loss = total_seg_loss / len(train_loader)
        avg_boundary_loss = total_boundary_loss / len(train_loader)
        avg_quality_loss = total_quality_loss / len(train_loader)
        
        return avg_loss, avg_seg_loss, avg_boundary_loss, avg_quality_loss
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc='Validation'):
                try:
                    images = batch['image'].to(self.device)
                    seg_masks = batch['seg_mask'].to(self.device)
                    rock_qualities = batch['rock_quality'].to(self.device)
                    
                    targets = {
                        'seg_mask': seg_masks,
                        'rock_quality': rock_qualities
                    }
                    
                    outputs = self.model(images)
                    loss_dict = self.criterion(outputs, targets)
                    
                    total_loss += loss_dict['total_loss'].item()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    _, predicted = torch.max(outputs['rock_quality'], 1)
                    correct_predictions += (predicted == rock_qualities).sum().item()
                    total_predictions += rock_qualities.size(0)
                    
                except Exception as e:
                    logger.error(f"éªŒè¯æ‰¹æ¬¡å‡ºé”™: {e}")
                    continue
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config.num_epochs):
            logger.info(f"Epoch {epoch+1}/{self.config.num_epochs}")
            
            # è®­ç»ƒ
            train_loss, train_seg_loss, train_boundary_loss, train_quality_loss = self.train_epoch(train_loader)
            
            # éªŒè¯
            if val_loader:
                val_loss, val_accuracy = self.validate(val_loader)
                
                logger.info(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step(val_loss)
                
                # æ—©åœæ£€æŸ¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    self.save_checkpoint(epoch, 'best_model.pth')
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config.patience:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}")
                    break
                
                # è®°å½•å†å²
                self.train_history['val_loss'].append(val_loss)
                self.train_history['val_accuracy'].append(val_accuracy)
            else:
                logger.info(f"Train Loss: {train_loss:.4f}")
            
            # è®°å½•è®­ç»ƒå†å²
            self.train_history['loss'].append(train_loss)
            self.train_history['seg_loss'].append(train_seg_loss)
            self.train_history['boundary_loss'].append(train_boundary_loss)
            self.train_history['quality_loss'].append(train_quality_loss)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch, f'checkpoint_epoch_{epoch+1}.pth')
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(self.config.num_epochs-1, 'final_model.pth')
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        logger.info("è®­ç»ƒå®Œæˆï¼")
    
    def save_checkpoint(self, epoch, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_history': self.train_history,
            'config': self.config
        }
        
        checkpoint_path = os.path.join(self.config.output_dir, 'checkpoints', filename)
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = os.path.join(self.config.output_dir, 'logs', 'training_history.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.train_history['loss'], label='Total Loss')
        axes[0, 0].plot(self.train_history['seg_loss'], label='Seg Loss')
        axes[0, 0].plot(self.train_history['quality_loss'], label='Quality Loss')
        if self.train_history['val_loss']:
            axes[0, 0].plot(self.train_history['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Training Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        if self.train_history['val_accuracy']:
            axes[0, 1].plot(self.train_history['val_accuracy'], label='Val Accuracy')
            axes[0, 1].set_title('Validation Accuracy')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        current_lr = [group['lr'] for group in self.optimizer.param_groups][0]
        axes[1, 0].axhline(y=current_lr, color='r', linestyle='--', label=f'Current LR: {current_lr:.2e}')
        axes[1, 0].set_title('Learning Rate')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # å›´å²©ç­‰çº§åˆ†å¸ƒï¼ˆç¤ºä¾‹ï¼‰
        axes[1, 1].bar(range(15), [1]*15)
        axes[1, 1].set_title('Rock Quality Distribution (15 Levels)')
        axes[1, 1].set_xlabel('Quality Level')
        axes[1, 1].set_ylabel('Count')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plot_path = os.path.join(self.config.output_dir, 'visualizations', 'training_curves.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")

def create_data_loaders(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # æ•°æ®å˜æ¢
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = TunnelDataset(
        coco_file=config.coco_file,
        images_dir=config.images_dir,
        target_size=config.input_size,
        transform=transform
    )
    
    # åˆ†å‰²æ•°æ®é›†
    if len(dataset) > 1:
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    else:
        train_dataset = dataset
        val_dataset = None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True if config.device.type == 'cuda' else False
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=config.num_workers,
            pin_memory=True if config.device.type == 'cuda' else False
        )
    
    return train_loader, val_loader

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("éš§é“æŒå­é¢å›´å²©ç­‰çº§æ™ºèƒ½è¯„ä¼°ç³»ç»Ÿï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰")
    print("åŠŸèƒ½ï¼šåœ°è´¨ç‰¹å¾åˆ†å‰² + å›´å²©ç­‰çº§æ™ºèƒ½é¢„æµ‹ï¼ˆ15çº§è¯¦ç»†åˆ†ç±»ï¼‰")
    print("åŸºäºCOCOæ ¼å¼æ ‡æ³¨æ•°æ®çš„ç«¯åˆ°ç«¯è®­ç»ƒ")
    print("="*60)
    
    try:
        # åˆ›å»ºé…ç½®
        config = RockQualityConfig()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_data_loaders(config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RockQualityTrainer(config)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train(train_loader, val_loader)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        generate_final_report(config, trainer)
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        print("\n" + "="*60)
        print("é”™è¯¯è§£å†³æ–¹æ¡ˆï¼š")
        print("1. æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œå­˜å‚¨ç©ºé—´")
        print("3. å¦‚æœæ˜¯CUDAé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨CPUè®­ç»ƒ")
        print("4. æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–åº“ç‰ˆæœ¬")
        print("="*60)

def generate_final_report(config, trainer):
    """ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
    report = {
        'training_config': {
            'num_epochs': config.num_epochs,
            'batch_size': config.batch_size,
            'learning_rate': config.learning_rate,
            'device': str(config.device),
            'num_quality_classes': config.num_quality_classes
        },
        'model_info': {
            'total_parameters': sum(p.numel() for p in trainer.model.parameters()),
            'trainable_parameters': sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
        },
        'training_results': trainer.train_history,
        'rock_quality_levels': config.rock_quality_levels,
        'output_files': {
            'checkpoints': os.path.join(config.output_dir, 'checkpoints'),
            'logs': os.path.join(config.output_dir, 'logs'),
            'visualizations': os.path.join(config.output_dir, 'visualizations'),
            'reports': os.path.join(config.output_dir, 'reports')
        }
    }
    
    report_path = os.path.join(config.output_dir, 'reports', 'final_training_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"æœ€ç»ˆè®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆæ‘˜è¦ï¼š")
    print(f"- å›´å²©ç­‰çº§åˆ†ç±»: 15çº§è¯¦ç»†åˆ†ç±»")
    print(f"- æ¨¡å‹å‚æ•°æ•°é‡: {report['model_info']['total_parameters']:,}")
    print(f"- è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"- æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®: {report['output_files']['checkpoints']}")
    print(f"- è®­ç»ƒæ—¥å¿—: {report['output_files']['logs']}")
    print(f"- å¯è§†åŒ–ç»“æœ: {report['output_files']['visualizations']}")
    print("="*60)
    
    # ä¿å­˜æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š
    final_report_path = os.path.join(config.output_dir, 'final_training_report.json')
    with open(final_report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"\næœ€ç»ˆè®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜è‡³: {final_report_path}")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("\nGPUå†…å­˜å·²æ¸…ç†")
    
    print("\nğŸ‰ è®­ç»ƒä»»åŠ¡å®Œæˆï¼")
    print(f"æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’")
    
    return report

if __name__ == "__main__":
    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è¿è¡Œä¸»è®­ç»ƒå‡½æ•°
        final_report = main()
        
        print("\n" + "="*80)
        print("ğŸš€ å›´å²©ç­‰çº§æ™ºèƒ½è¯„ä¼°æ¨¡å‹è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("="*80)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        sys.exit(0)
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        sys.exit(1)